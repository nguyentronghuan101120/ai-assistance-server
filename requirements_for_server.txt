fastapi[standard] == 0.114.0
uvicorn == 0.34.2

# If use diffusers
diffusers == 0.33.1
accelerate == 1.6.0
transformers == 4.52.4

# # Offline install
# torch==2.7.0
# -f ./local_packages/torch-2.7.0+cu128-cp310-cp310-win_amd64.whl
# bitsandbytes -f ./local_packages/bitsandbytes-0.46.0-cp310-cp310-win_amd64.whl

# If use llama-cpp-python
# llama-cpp-python == 0.3.8
# --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124

# If process file feature enable
# beautifulsoup4 == 4.13.4
# requests == 2.32.3
# langchain_chroma == 0.2.2
# langchain_huggingface == 0.1.2
# langchain_community == 0.3.19
# chromadb == 0.6.3
# pymupdf == 1.25.1

